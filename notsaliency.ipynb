{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "91dca4bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== リアルタイム文字領域検出システム ===\n",
      "ウェブカメラを使用して文字領域をリアルタイム検出します\n",
      "カメラ初期化完了:\n",
      "  解像度: 640x480\n",
      "  FPS: 30\n",
      "\n",
      "リアルタイム処理開始\n",
      "操作方法:\n",
      "  1-4キー: 表示モード切り替え\n",
      "  Sキー: 現在のフレームを保存\n",
      "  Fキー: FPS表示ON/OFF\n",
      "  Qキー: 終了\n",
      "--------------------------------------------------\n",
      "リソースを解放しました\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import time\n",
    "import threading\n",
    "from queue import Queue\n",
    "\n",
    "class RealTimeShapeTextExtractor:\n",
    "    \"\"\"\n",
    "    ウェブカメラからのリアルタイム映像で概形と文字領域を検出するクラス\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, camera_id=0, process_width=400, display_width=800):\n",
    "        # カメラ設定\n",
    "        self.camera_id = camera_id\n",
    "        self.cap = None\n",
    "        \n",
    "        # 処理サイズ設定（処理は小さいサイズ、表示は大きいサイズ）\n",
    "        self.process_width = process_width\n",
    "        self.display_width = display_width\n",
    "        \n",
    "        # 検出パラメータ\n",
    "        self.blur_ksize = (9, 9)\n",
    "        self.canny_thresh1 = 50\n",
    "        self.canny_thresh2 = 150\n",
    "        self.min_text_area = 30\n",
    "        self.max_text_area = 5000\n",
    "        self.min_aspect_ratio = 0.1\n",
    "        self.max_aspect_ratio = 8\n",
    "        \n",
    "        # 処理制御\n",
    "        self.process_every_n_frames = 3  # N フレームごとに処理\n",
    "        self.frame_count = 0\n",
    "        self.last_detection_result = None\n",
    "        \n",
    "        # フレームレート計算用\n",
    "        self.fps_counter = 0\n",
    "        self.fps_start_time = time.time()\n",
    "        self.current_fps = 0\n",
    "        \n",
    "        # 表示モード\n",
    "        self.display_mode = 0  # 0: 原画像, 1: 概形, 2: 文字検出結果, 3: 二値化画像\n",
    "        self.show_fps = True\n",
    "        self.show_detection_info = True\n",
    "        \n",
    "        # 処理状態\n",
    "        self.processing = False\n",
    "        self.last_shape_mask = None\n",
    "        \n",
    "    def initialize_camera(self):\n",
    "        \"\"\"カメラの初期化\"\"\"\n",
    "        self.cap = cv2.VideoCapture(self.camera_id)\n",
    "        if not self.cap.isOpened():\n",
    "            raise RuntimeError(f\"カメラ {self.camera_id} を開けませんでした\")\n",
    "        \n",
    "        # カメラ設定の最適化\n",
    "        self.cap.set(cv2.CAP_PROP_FRAME_WIDTH, 640)\n",
    "        self.cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 480)\n",
    "        self.cap.set(cv2.CAP_PROP_FPS, 30)\n",
    "        self.cap.set(cv2.CAP_PROP_BUFFERSIZE, 1)  # バッファサイズを小さくして遅延を減らす\n",
    "        \n",
    "        print(f\"カメラ初期化完了:\")\n",
    "        print(f\"  解像度: {int(self.cap.get(cv2.CAP_PROP_FRAME_WIDTH))}x{int(self.cap.get(cv2.CAP_PROP_FRAME_HEIGHT))}\")\n",
    "        print(f\"  FPS: {int(self.cap.get(cv2.CAP_PROP_FPS))}\")\n",
    "    \n",
    "    def resize_frame(self, frame, target_width):\n",
    "        \"\"\"フレームをリサイズ\"\"\"\n",
    "        h, w = frame.shape[:2]\n",
    "        ratio = target_width / w\n",
    "        new_height = int(h * ratio)\n",
    "        return cv2.resize(frame, (target_width, new_height)), ratio\n",
    "    \n",
    "    def extract_shape_fast(self, frame):\n",
    "        \"\"\"高速な概形抽出\"\"\"\n",
    "        # 処理用サイズにリサイズ\n",
    "        small_frame, ratio = self.resize_frame(frame, self.process_width)\n",
    "        gray = cv2.cvtColor(small_frame, cv2.COLOR_BGR2GRAY)\n",
    "        \n",
    "        # 高速ブラー\n",
    "        blurred = cv2.GaussianBlur(gray, self.blur_ksize, 0)\n",
    "        \n",
    "        # Canny エッジ検出\n",
    "        edges = cv2.Canny(blurred, self.canny_thresh1, self.canny_thresh2)\n",
    "        \n",
    "        # 軽量なモルフォロジー演算\n",
    "        kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (3, 3))\n",
    "        edges = cv2.morphologyEx(edges, cv2.MORPH_CLOSE, kernel)\n",
    "        \n",
    "        # 輪郭抽出\n",
    "        contours, _ = cv2.findContours(edges, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "        ###############################################################################ここまででデバッグ\n",
    "        mask = np.zeros_like(gray)\n",
    "        if contours:\n",
    "            # 面積上位3つの輪郭を考慮（より安定した検出）\n",
    "            contours = sorted(contours, key=cv2.contourArea, reverse=True)[:5]\n",
    "            for cnt in contours:\n",
    "                if cv2.contourArea(cnt) > 1000:  # 最小面積フィルタ\n",
    "                    cv2.drawContours(mask, [cnt], -1, 255, -1)\n",
    "        \n",
    "        # 元のサイズにリサイズ\n",
    "        h, w = frame.shape[:2]\n",
    "        mask_resized = cv2.resize(mask, (w, h))\n",
    "        shape_img = cv2.bitwise_and(frame, frame, mask=mask_resized)\n",
    "        \n",
    "        return shape_img, mask_resized, edges\n",
    "    \n",
    "    def extract_text_regions_fast(self, frame, shape_mask=None):\n",
    "        \"\"\"高速な文字領域抽出\"\"\"\n",
    "        # 概形マスクがある場合は適用\n",
    "        if shape_mask is not None:\n",
    "            frame = cv2.bitwise_and(frame, frame, mask=shape_mask)\n",
    "        \n",
    "        # 処理用サイズにリサイズ\n",
    "        small_frame, ratio = self.resize_frame(frame, self.process_width)\n",
    "        gray = cv2.cvtColor(small_frame, cv2.COLOR_BGR2GRAY)\n",
    "        \n",
    "        # 高速二値化\n",
    "        _, binary = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)\n",
    "        \n",
    "        # 軽量なノイズ除去\n",
    "        kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (2, 2))\n",
    "        binary = cv2.morphologyEx(binary, cv2.MORPH_CLOSE, kernel)\n",
    "        \n",
    "        # 輪郭抽出\n",
    "        contours, _ = cv2.findContours(binary, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "        \n",
    "        text_boxes = []\n",
    "        for cnt in contours:\n",
    "            x, y, w, h = cv2.boundingRect(cnt)\n",
    "            area = w * h\n",
    "            aspect_ratio = w / h if h > 0 else 0\n",
    "            \n",
    "            # 文字領域判定（より緩い条件で高速化）\n",
    "            if (self.min_text_area < area < self.max_text_area and \n",
    "                h > 6 and w > 4 and \n",
    "                self.min_aspect_ratio < aspect_ratio < self.max_aspect_ratio):\n",
    "                \n",
    "                # 元のサイズに座標を変換\n",
    "                x_orig = int(x / ratio)\n",
    "                y_orig = int(y / ratio)\n",
    "                w_orig = int(w / ratio)\n",
    "                h_orig = int(h / ratio)\n",
    "                \n",
    "                text_boxes.append((x_orig, y_orig, w_orig, h_orig))\n",
    "        \n",
    "        return text_boxes, binary\n",
    "    \n",
    "    def draw_results(self, frame, text_boxes, fps, detection_count):\n",
    "        \"\"\"検出結果を描画\"\"\"\n",
    "        result_frame = frame.copy()\n",
    "        \n",
    "        # 文字領域を描画\n",
    "        for i, (x, y, w, h) in enumerate(text_boxes):\n",
    "            cv2.rectangle(result_frame, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "            # テキストボックスの番号を表示（小さく）\n",
    "            if len(text_boxes) < 20:  # 多すぎる場合は番号を省略\n",
    "                cv2.putText(result_frame, f'{i+1}', (x, y-5), \n",
    "                           cv2.FONT_HERSHEY_SIMPLEX, 0.4, (0, 255, 0), 1)\n",
    "        \n",
    "        # 情報表示\n",
    "        if self.show_fps:\n",
    "            cv2.putText(result_frame, f'FPS: {fps:.1f}', (10, 30), \n",
    "                       cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 255), 2)\n",
    "        \n",
    "        if self.show_detection_info:\n",
    "            cv2.putText(result_frame, f'Text Regions: {detection_count}', (10, 60), \n",
    "                       cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 255), 2)\n",
    "        \n",
    "        # 操作ガイド\n",
    "        mode_names = ['Original', 'Shape', 'Detection', 'Binary']\n",
    "        cv2.putText(result_frame, f'Mode: {mode_names[self.display_mode]} (Press 1-4)', \n",
    "                   (10, result_frame.shape[0] - 60), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)\n",
    "        cv2.putText(result_frame, 'Press Q to quit, S to save, F to toggle FPS', \n",
    "                   (10, result_frame.shape[0] - 30), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)\n",
    "        \n",
    "        return result_frame\n",
    "    \n",
    "    def calculate_fps(self):\n",
    "        \"\"\"FPS計算\"\"\"\n",
    "        self.fps_counter += 1\n",
    "        if self.fps_counter % 10 == 0:  # 10フレームごとに更新\n",
    "            current_time = time.time()\n",
    "            self.current_fps = 10 / (current_time - self.fps_start_time)\n",
    "            self.fps_start_time = current_time\n",
    "    \n",
    "    def save_current_frame(self, frame, text_boxes):\n",
    "        \"\"\"現在のフレームを保存\"\"\"\n",
    "        timestamp = time.strftime(\"%Y%m%d_%H%M%S\")\n",
    "        \n",
    "        # 検出結果付きの画像を保存\n",
    "        result_frame = self.draw_results(frame, text_boxes, self.current_fps, len(text_boxes))\n",
    "        filename = f\"capture_{timestamp}.png\"\n",
    "        cv2.imwrite(filename, result_frame)\n",
    "        \n",
    "        print(f\"フレームを保存しました: {filename}\")\n",
    "        print(f\"検出された文字領域数: {len(text_boxes)}\")\n",
    "    \n",
    "    def run(self):\n",
    "        \"\"\"メインループ\"\"\"\n",
    "        try:\n",
    "            self.initialize_camera()\n",
    "            print(\"\\nリアルタイム処理開始\")\n",
    "            print(\"操作方法:\")\n",
    "            print(\"  1-4キー: 表示モード切り替え\")\n",
    "            print(\"  Sキー: 現在のフレームを保存\")\n",
    "            print(\"  Fキー: FPS表示ON/OFF\")\n",
    "            print(\"  Qキー: 終了\")\n",
    "            print(\"-\" * 50)\n",
    "            \n",
    "            while True:\n",
    "                ret, frame = self.cap.read()\n",
    "                if not ret:\n",
    "                    print(\"フレーム読み取りエラー\")\n",
    "                    break\n",
    "                \n",
    "                # フレームカウント\n",
    "                self.frame_count += 1\n",
    "                self.calculate_fps()\n",
    "                \n",
    "                # 処理するフレームを間引き\n",
    "                if self.frame_count % self.process_every_n_frames == 0:\n",
    "                    self.processing = True\n",
    "                    \n",
    "                    # 概形抽出\n",
    "                    shape_img, shape_mask, edges = self.extract_shape_fast(frame)\n",
    "                    self.last_shape_mask = shape_mask\n",
    "                    \n",
    "                    # 文字領域抽出\n",
    "                    text_boxes, binary = self.extract_text_regions_fast(frame, shape_mask)\n",
    "                    \n",
    "                    # 結果を保存\n",
    "                    self.last_detection_result = {\n",
    "                        'shape_img': shape_img,\n",
    "                        'text_boxes': text_boxes,\n",
    "                        'binary': binary,\n",
    "                        'edges': edges\n",
    "                    }\n",
    "                    \n",
    "                    self.processing = False\n",
    "                \n",
    "                # 表示フレーム準備\n",
    "                display_frame, _ = self.resize_frame(frame, self.display_width)\n",
    "                \n",
    "                # 表示モードに応じて画像を選択\n",
    "                if self.last_detection_result is not None:\n",
    "                    result = self.last_detection_result\n",
    "                    \n",
    "                    if self.display_mode == 0:  # 原画像 + 検出結果\n",
    "                        show_frame = self.draw_results(display_frame, \n",
    "                                                     self._scale_boxes(result['text_boxes'], \n",
    "                                                                     frame.shape[1], display_frame.shape[1]), \n",
    "                                                     self.current_fps, len(result['text_boxes']))\n",
    "                    elif self.display_mode == 1:  # 概形\n",
    "                        shape_resized, _ = self.resize_frame(result['shape_img'], self.display_width)\n",
    "                        show_frame = self.draw_results(shape_resized, \n",
    "                                                     self._scale_boxes(result['text_boxes'], \n",
    "                                                                     frame.shape[1], shape_resized.shape[1]), \n",
    "                                                     self.current_fps, len(result['text_boxes']))\n",
    "                    elif self.display_mode == 2:  # 検出結果のみ\n",
    "                        show_frame = self.draw_results(display_frame, \n",
    "                                                     self._scale_boxes(result['text_boxes'], \n",
    "                                                                     frame.shape[1], display_frame.shape[1]), \n",
    "                                                     self.current_fps, len(result['text_boxes']))\n",
    "                    elif self.display_mode == 3:  # 二値化画像\n",
    "                        binary_resized = cv2.resize(result['binary'], (self.display_width, \n",
    "                                                   int(result['binary'].shape[0] * self.display_width / result['binary'].shape[1])))\n",
    "                        show_frame = cv2.cvtColor(binary_resized, cv2.COLOR_GRAY2BGR)\n",
    "                        if self.show_fps:\n",
    "                            cv2.putText(show_frame, f'FPS: {self.current_fps:.1f}', (10, 30), \n",
    "                                       cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 255), 2)\n",
    "                else:\n",
    "                    show_frame = display_frame\n",
    "                \n",
    "                # 画像表示\n",
    "                cv2.imshow('Real-time Shape & Text Detection', show_frame)\n",
    "                \n",
    "                # キー入力処理\n",
    "                key = cv2.waitKey(1) & 0xFF\n",
    "                if key == ord('q') or key == ord('Q'):\n",
    "                    break\n",
    "                elif key == ord('1'):\n",
    "                    self.display_mode = 0\n",
    "                elif key == ord('2'):\n",
    "                    self.display_mode = 1\n",
    "                elif key == ord('3'):\n",
    "                    self.display_mode = 2\n",
    "                elif key == ord('4'):\n",
    "                    self.display_mode = 3\n",
    "                elif key == ord('s') or key == ord('S'):\n",
    "                    if self.last_detection_result is not None:\n",
    "                        self.save_current_frame(frame, self.last_detection_result['text_boxes'])\n",
    "                elif key == ord('f') or key == ord('F'):\n",
    "                    self.show_fps = not self.show_fps\n",
    "                elif key == ord('h') or key == ord('H'):\n",
    "                    self.show_detection_info = not self.show_detection_info\n",
    "        \n",
    "        except KeyboardInterrupt:\n",
    "            print(\"\\n処理を中断しました\")\n",
    "        except Exception as e:\n",
    "            print(f\"エラーが発生しました: {e}\")\n",
    "        finally:\n",
    "            self.cleanup()\n",
    "    \n",
    "    def _scale_boxes(self, boxes, original_width, target_width):\n",
    "        \"\"\"ボックス座標をスケール変換\"\"\"\n",
    "        if not boxes:\n",
    "            return []\n",
    "        \n",
    "        scale = target_width / original_width\n",
    "        scaled_boxes = []\n",
    "        for x, y, w, h in boxes:\n",
    "            scaled_boxes.append((int(x * scale), int(y * scale), \n",
    "                               int(w * scale), int(h * scale)))\n",
    "        return scaled_boxes\n",
    "    \n",
    "    def cleanup(self):\n",
    "        \"\"\"リソースの解放\"\"\"\n",
    "        if self.cap is not None:\n",
    "            self.cap.release()\n",
    "        cv2.destroyAllWindows()\n",
    "        print(\"リソースを解放しました\")\n",
    "\n",
    "class WebcamTextDetector:\n",
    "    \"\"\"簡単な使用のためのラッパークラス\"\"\"\n",
    "    \n",
    "    def __init__(self, camera_id=0):\n",
    "        self.detector = RealTimeShapeTextExtractor(camera_id=camera_id)\n",
    "    \n",
    "    def start(self):\n",
    "        \"\"\"検出開始\"\"\"\n",
    "        self.detector.run()\n",
    "    \n",
    "    def set_sensitivity(self, sensitivity='medium'):\n",
    "        \"\"\"検出感度の設定\"\"\"\n",
    "        if sensitivity == 'high':\n",
    "            self.detector.min_text_area = 20\n",
    "            self.detector.max_text_area = 8000\n",
    "            self.detector.process_every_n_frames = 2\n",
    "        elif sensitivity == 'medium':\n",
    "            self.detector.min_text_area = 30\n",
    "            self.detector.max_text_area = 5000\n",
    "            self.detector.process_every_n_frames = 3\n",
    "        elif sensitivity == 'low':\n",
    "            self.detector.min_text_area = 50\n",
    "            self.detector.max_text_area = 3000\n",
    "            self.detector.process_every_n_frames = 4\n",
    "\n",
    "def main():\n",
    "    \"\"\"メイン関数\"\"\"\n",
    "    print(\"=== リアルタイム文字領域検出システム ===\")\n",
    "    print(\"ウェブカメラを使用して文字領域をリアルタイム検出します\")\n",
    "    \n",
    "    try:\n",
    "        # 検出器の作成と開始\n",
    "        detector = WebcamTextDetector(camera_id=0)  # 0 = デフォルトカメラ\n",
    "        \n",
    "        # 感度設定（オプション）\n",
    "        detector.set_sensitivity('medium')  # 'high', 'medium', 'low'\n",
    "        \n",
    "        # 検出開始\n",
    "        detector.start()\n",
    "        \n",
    "    except RuntimeError as e:\n",
    "        print(f\"カメラエラー: {e}\")\n",
    "        print(\"別のカメラIDを試してみてください（例: camera_id=1）\")\n",
    "    except Exception as e:\n",
    "        print(f\"予期しないエラー: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n",
    "\n",
    "#ウィークポイントの検討\n",
    "#環境条件のロバスト性\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7f5be176",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ウェブカメラからのリアルタイム文字様領域抽出を開始します。\n",
      " 'q' キーを押すと終了します。\n",
      "プログラムを終了しました。\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import json\n",
    "import time\n",
    "\n",
    "# --- 設定 ---\n",
    "# ウェブカメラのID (通常は0)\n",
    "WEBCAM_ID = 0\n",
    "# フレーム処理の遅延時間 (ミリ秒) - リアルタイム性を調整\n",
    "FRAME_DELAY_MS = 1\n",
    "# 検出されたバウンディングボックスを描画する色 (BGR形式) - 今回はマスクに使用\n",
    "BBOX_COLOR = (0, 255, 0) # 緑色\n",
    "# バウンディングボックスの線の太さ\n",
    "BBOX_THICKNESS = 2\n",
    "\n",
    "# --- DBNetモデルの概念的なロードと初期化のプレースホルダー ---\n",
    "# 実際のアプリケーションでは、ここにDBNetモデルをロードするコードが記述されます。\n",
    "# 例:\n",
    "# import torch\n",
    "# import onnxruntime # ONNX形式のモデルを使用する場合\n",
    "#\n",
    "# try:\n",
    "#     # 事前学習済みDBNetモデルのパスを指定\n",
    "#     # このモデルファイルは、環境に事前に配置されている必要があります。\n",
    "#     DBNET_MODEL_PATH = \"path/to/your/dbnet_model.pth\" # PyTorchモデルの場合\n",
    "#     # DBNET_MODEL_PATH = \"path/to/your/dbnet_model.onnx\" # ONNXモデルの場合\n",
    "#\n",
    "#     # モデルのロード (例: PyTorch)\n",
    "#     # dbnet_model = torch.load(DBNET_MODEL_PATH, map_location=torch.device('cpu'))\n",
    "#     # dbnet_model.eval() # 推論モードに設定\n",
    "#\n",
    "#     # またはONNX Runtimeセッションの作成 (ONNXモデルの場合)\n",
    "#     # sess_options = onnxruntime.SessionOptions()\n",
    "#     # dbnet_session = onnxruntime.InferenceSession(DBNET_MODEL_PATH, sess_options)\n",
    "#     # input_name = dbnet_session.get_inputs()[0].name\n",
    "#     # output_name = dbnet_session.get_outputs()[0].name\n",
    "#\n",
    "#     print(\"DBNetモデルのロードを概念的に完了しました。\")\n",
    "# except Exception as e:\n",
    "#     print(f\"DBNetモデルのロード中にエラーが発生しました (シミュレーション): {e}\")\n",
    "#     # 実際のアプリケーションでは、ここでエラーハンドリングを行います。\n",
    "#\n",
    "# --- グローバルからローカルへの概念的な処理のためのプレースホルダー関数 ---\n",
    "# 実際のシステムでは、この関数内で文書レイアウト解析（DLA）モデルが実行され、\n",
    "# 文書の「概形」（例：列、テキストブロック、画像領域）を識別します。\n",
    "# その後、simulate_dbnet_text_detection がこれらの概形領域内で詳細な文字を検出します。\n",
    "def identify_global_document_shapes(frame):\n",
    "    \"\"\"\n",
    "    フレームから主要な文書の「概形」を識別する概念的な関数。\n",
    "    実際のアプリケーションでは、Recursive XY Cut (RXYC) や深層学習ベースの\n",
    "    レイアウトセグメンテーションモデルがここに統合されます。\n",
    "    このデモンストレーションでは、簡略化のためにフレーム全体を単一の「概形」として扱います。\n",
    "    \"\"\"\n",
    "    # フレーム全体を単一のグローバル領域として返します。\n",
    "    # 実際のDLAでは、複数の異なる領域（例：[x, y, w, h, 'text_column'], [x, y, w, h, 'image_area']）\n",
    "    # が返されるでしょう。\n",
    "    height, width, _ = frame.shape\n",
    "    return [{'bbox': [0, 0, width, height], 'type': 'full_frame', 'id': 'global_0'}]\n",
    "\n",
    "# --- 文字のような領域を検出する関数 (DBNetの動作をシミュレーション) ---\n",
    "def simulate_dbnet_text_detection(frame, global_region):\n",
    "    \"\"\"\n",
    "    指定されたグローバル領域内で文字のような領域を検出するシミュレーション関数。\n",
    "    NOTE: 実際のアプリケーションでは、この関数はロードされたDBNetモデルを使用して\n",
    "    テキスト検出推論を実行します。現在のOpenCVベースのロジックは、\n",
    "    DBNetが達成するであろう「文字らしい領域の抽出」を概念的にシミュレートするものです。\n",
    "\n",
    "    DBNetの一般的な処理フロー:\n",
    "    1. 前処理: 入力画像をDBNetモデルの入力要件に合わせてリサイズ、正規化します。\n",
    "    2. 推論: 前処理された画像をDBNetモデルに入力し、確率マップやバイナリマップなどの出力を取得します。\n",
    "    3. 後処理: モデルの出力からテキスト領域のバウンディングボックス (またはポリゴン) を抽出します。\n",
    "       これには、Differentiable Binarizationの逆操作や、非最大抑制 (NMS) などが含まれます。\n",
    "\n",
    "    Args:\n",
    "        frame (np.array): 入力画像フレーム。\n",
    "        global_region (dict): 処理するグローバル領域のバウンディングボックス情報を含む辞書。\n",
    "\n",
    "    Returns:\n",
    "        list: 検出された文字のような領域のバウンディングボックスと信頼度スコアのリスト。\n",
    "              例: [{'bbox': [x, y, w, h], 'confidence': 0.9}, ...]\n",
    "    \"\"\"\n",
    "    # グローバル領域の座標を抽出\n",
    "    x, y, w, h = global_region['bbox']\n",
    "    \n",
    "    # グローバル領域に基づいてフレームを切り抜き (実際のDLAではこの切り抜きが重要)\n",
    "    cropped_frame = frame[y:y+h, x:x+w]\n",
    "    if cropped_frame.shape[0] == 0 or cropped_frame.shape[1] == 0:\n",
    "        return [] # 無効な切り抜きの場合は空リストを返す\n",
    "\n",
    "    # --- ここからDBNetの「推論結果」をシミュレートするOpenCVロジック ---\n",
    "    # 実際のDBNetでは、これらのOpenCV操作はDBNetモデルの内部で学習された特徴抽出と\n",
    "    # テキスト領域のセグメンテーションに置き換えられます。\n",
    "    \n",
    "    # 1. グレースケールに変換\n",
    "    gray = cv2.cvtColor(cropped_frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # 2. 適応的二値化 (様々な照明条件に対応するため)\n",
    "    # Otsu's binarization を使用してテキストと背景を分離\n",
    "    _, binary = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)\n",
    "\n",
    "    # 3. モルフォロジー変換 (文字を結合して単語や行にする)\n",
    "    # 適切なカーネルサイズはテキストのサイズと密度に依存します。\n",
    "    kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (5, 5))\n",
    "    dilated = cv2.dilate(binary, kernel, iterations=1)\n",
    "\n",
    "    # 4. 輪郭の検出\n",
    "    contours, _ = cv2.findContours(dilated, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "    detected_regions = []\n",
    "    for contour in contours:\n",
    "        # 輪郭からバウンディングボックスを取得\n",
    "        cx, cy, cw, ch = cv2.boundingRect(contour)\n",
    "\n",
    "        # --- フィルタリング条件の強化 ---\n",
    "        # これらの閾値は、検出したい文字のサイズと形状に合わせて調整する必要があります。\n",
    "        # 小さすぎる領域や、極端なアスペクト比、ソリディティの低い領域を除外 (ノイズフィルタリング)\n",
    "        \n",
    "        # 輪郭の面積を計算\n",
    "        area = cv2.contourArea(contour)\n",
    "        \n",
    "        # 最小面積の閾値 (調整)\n",
    "        min_area = 100 \n",
    "        if area < min_area:\n",
    "            continue\n",
    "\n",
    "        # アスペクト比のチェック (幅/高さ)\n",
    "        min_aspect_ratio = 0.1\n",
    "        max_aspect_ratio = 8.0 \n",
    "        if ch > 0: # 高さで割る前にゼロ除算を避ける\n",
    "            aspect_ratio = float(cw) / ch\n",
    "            if not (min_aspect_ratio < aspect_ratio < max_aspect_ratio):\n",
    "                continue\n",
    "\n",
    "        # ソリディティのチェック (輪郭の面積 / その凸包の面積)\n",
    "        hull = cv2.convexHull(contour)\n",
    "        hull_area = cv2.contourArea(hull)\n",
    "        if hull_area > 0: # ゼロ除算を避ける\n",
    "            solidity = float(area) / hull_area\n",
    "            min_solidity = 0.4 \n",
    "            if solidity < min_solidity:\n",
    "                continue\n",
    "        else:\n",
    "            continue # 凸包の面積が0の場合は無効な輪郭とみなす\n",
    "\n",
    "        # --- フィルタリング条件の強化ここまで ---\n",
    "\n",
    "        # グローバル領域に対する相対座標を元のフレーム座標に変換\n",
    "        original_x = x + cx\n",
    "        original_y = y + cy\n",
    "\n",
    "        detected_regions.append({\n",
    "            'bbox': [original_x, original_y, cw, ch],\n",
    "            'confidence': 0.95 # シミュレーションのため固定値\n",
    "        })\n",
    "    \n",
    "    return detected_regions\n",
    "\n",
    "# --- メイン処理 ---\n",
    "def main():\n",
    "    cap = cv2.VideoCapture(WEBCAM_ID)\n",
    "\n",
    "    if not cap.isOpened():\n",
    "        print(f\"エラー: ウェブカメラ (ID: {WEBCAM_ID}) を開けませんでした。\")\n",
    "        return\n",
    "\n",
    "    print(\"ウェブカメラからのリアルタイム文字様領域抽出を開始します。\")\n",
    "    print(\" 'q' キーを押すと終了します。\")\n",
    "\n",
    "    frame_count = 0\n",
    "    start_time = time.time()\n",
    "\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            print(\"フレームを読み取れませんでした。終了します。\")\n",
    "            break\n",
    "\n",
    "        frame_count += 1\n",
    "\n",
    "        # 1. グローバルレイアウト解析 (概形認識)\n",
    "        global_regions = identify_global_document_shapes(frame)\n",
    "        \n",
    "        all_detected_text_like_regions = []\n",
    "\n",
    "        # 2. 各グローバル領域内でローカルな文字様要素を検出 (詳細情報認識)\n",
    "        # ここで、DBNetモデルがロードされている場合、その推論が実行されます。\n",
    "        for g_region in global_regions:\n",
    "            # 実際のDBNet推論の呼び出しをシミュレート\n",
    "            detected_local_regions = simulate_dbnet_text_detection(frame, g_region)\n",
    "            all_detected_text_like_regions.extend(detected_local_regions)\n",
    "            \n",
    "        # 検出された文字様領域のみを抽出するためのマスクを作成\n",
    "        height, width, _ = frame.shape\n",
    "        mask = np.zeros((height, width, 3), dtype=np.uint8)\n",
    "\n",
    "        # 検出された各文字様領域をマスク上で白く塗りつぶす\n",
    "        for region in all_detected_text_like_regions:\n",
    "            x, y, w, h = region['bbox']\n",
    "            # 矩形領域を白 (255, 255, 255) で塗りつぶす\n",
    "            cv2.rectangle(mask, (x, y), (x + w, y + h), (255, 255, 255), -1) # -1 は塗りつぶしを意味する\n",
    "\n",
    "        # 元のフレームとマスクをAND演算して、文字様領域のみを抽出\n",
    "        result_frame = cv2.bitwise_and(frame, mask)\n",
    "\n",
    "        # フレームレートの計算と表示\n",
    "        elapsed_time = time.time() - start_time\n",
    "        fps = frame_count / elapsed_time if elapsed_time > 0 else 0\n",
    "        cv2.putText(result_frame, f\"FPS: {fps:.2f}\", (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 255), 2)\n",
    "        \n",
    "        # 結果を表示\n",
    "        cv2.imshow('リアルタイム文字様領域抽出 (黒塗り強調 - DBNet概念導入)', result_frame)\n",
    "\n",
    "        # 'q' キーが押されたらループを終了\n",
    "        if cv2.waitKey(FRAME_DELAY_MS) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    # リソースを解放\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    print(\"プログラムを終了しました。\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
